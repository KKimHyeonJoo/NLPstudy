{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Thm_ZG0WcTpC"},"source":["# [자연어처리]\n","# 11주차(11-2). 언어 모델과 NNLM"]},{"cell_type":"markdown","source":["# 202002961 김현주\n","* **모든 셀 실행** 후 제출하시기 바랍니다.\n","* **실습 (11-2-1)**이 있습니다."],"metadata":{"id":"EQjjg1vRuQiB"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"emx0vJVr4_vi"}},{"cell_type":"code","source":["from collections import Counter"],"metadata":{"id":"VYSD1vU0TwAT","executionInfo":{"status":"ok","timestamp":1731864539248,"user_tz":-540,"elapsed":1011,"user":{"displayName":"ᄒᄒ","userId":"18137854489485594855"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# 코퍼스 정의\n","corpus = [\n","    \"I eat a strawberry\",\n","    \"I eat a blueberry\",\n","    \"I eat a strawberry cake\"\n","]"],"metadata":{"id":"b8ZmL5BGUG6X","executionInfo":{"status":"ok","timestamp":1731864539249,"user_tz":-540,"elapsed":4,"user":{"displayName":"ᄒᄒ","userId":"18137854489485594855"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"J_-P8AqXG4By"}},{"cell_type":"markdown","source":["# 통계 기반의 방법 활용 (10주차)\n","## 1-1. bigram (2-gram) 확률 계산"],"metadata":{"id":"2WLgKhYw90ai"}},{"cell_type":"code","source":["# 바이그램 모델 구축\n","def train_bigram_model(corpus):\n","    words = [word for sentence in corpus for word in sentence.split()]\n","    unigrams = Counter(words)\n","    bigrams = Counter(zip(words, words[1:]))\n","\n","    # 바이그램 확률 계산 (스무딩 없이)\n","    bigram_probabilities = {}\n","    for bigram, count in bigrams.items():\n","        bigram_probabilities[bigram] = count / unigrams[bigram[0]]\n","\n","    return bigram_probabilities, unigrams"],"metadata":{"id":"qNndyySpUI6s","executionInfo":{"status":"ok","timestamp":1731864539249,"user_tz":-540,"elapsed":4,"user":{"displayName":"ᄒᄒ","userId":"18137854489485594855"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# 모델 학습 (스무딩 없이)\n","bigram_probabilities, unigrams = train_bigram_model(corpus)"],"metadata":{"id":"ySp8GMrwUOkA","executionInfo":{"status":"ok","timestamp":1731864539249,"user_tz":-540,"elapsed":4,"user":{"displayName":"ᄒᄒ","userId":"18137854489485594855"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## P(cake|blueberry)를 계산해보자!\n","- 단순 카운트(방법 1-1)를 이용한 계산"],"metadata":{"id":"mfI7-yUCE2JI"}},{"cell_type":"code","source":["# 사용자로부터 단어 입력받아 확률 계산\n","w1 = 'blueberry' # input(\"Enter the first word: \")\n","w2 = 'cake' # input(\"Enter the second word: \")\n","bigram = (w1, w2)\n","\n","# 확률 조회\n","if bigram in bigram_probabilities:\n","    probability = bigram_probabilities[bigram]\n","else:\n","    # 스무딩 없이 확률을 0으로 처리\n","    probability = 0\n","\n","print(f\"P({w2} | {w1}) without Laplace Smoothing is: {probability}\")"],"metadata":{"id":"bpsA1oXhURP7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731864543608,"user_tz":-540,"elapsed":302,"user":{"displayName":"ᄒᄒ","userId":"18137854489485594855"}},"outputId":"52e59c04-c2ec-4595-b822-f7f99ef32f5b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["P(cake | blueberry) without Laplace Smoothing is: 0\n"]}]},{"cell_type":"markdown","source":["'blueberry cake'라는 bigram이 corpus에 존재하지 않기 때문에, 스무딩과 같은 방법을 적용하지 않을 경우 값이 0이됨."],"metadata":{"id":"rOD76Fep4TsD"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"9Oe0z6UlG47A"}},{"cell_type":"markdown","source":["## 1-2. 코퍼스에 등장하지 않았던 bigram을 계산하는 방법\n","- 라플라스 스무딩 적용"],"metadata":{"id":"II80x8rt-DFm"}},{"cell_type":"code","source":["# 중복된 코드이므로 주석 처리됨\n","# from collections import Counter, defaultdict"],"metadata":{"id":"HlYdkV4nVtEt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 중복된 코드이므로 주석 처리됨\n","# 코퍼스 정의\n","# corpus = [\n","#     \"I eat a strawberry\",\n","#     \"I eat a blueberry\",\n","#     \"I eat a strawberry cake\"\n","# ]"],"metadata":{"id":"ij8CR-8EU3A1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 바이그램 모델 구축\n","def train_bigram_model(corpus, alpha=1, use_smoothing=True):\n","    words = [word for sentence in corpus for word in sentence.split()]\n","    unigrams = Counter(words)\n","    bigrams = Counter(zip(words, words[1:]))\n","    vocabulary_size = len(unigrams)\n","\n","    # 바이그램 확률 계산\n","    bigram_probabilities = {}\n","    for bigram, count in bigrams.items():\n","        if use_smoothing:\n","            bigram_probabilities[bigram] = (count + alpha) / (unigrams[bigram[0]] + alpha * vocabulary_size)\n","        else:\n","            bigram_probabilities[bigram] = count / unigrams[bigram[0]]\n","\n","    return bigram_probabilities, unigrams, vocabulary_size"],"metadata":{"id":"TMagQeR2U44r","executionInfo":{"status":"ok","timestamp":1731864553811,"user_tz":-540,"elapsed":295,"user":{"displayName":"ᄒᄒ","userId":"18137854489485594855"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# 모델 학습 (라플라스 스무딩 사용 여부 선택 가능)\n","use_smoothing = \"yes\" # input(\"Use Laplace Smoothing? (yes/no): \").strip().lower() == \"yes\"\n","bigram_probabilities, unigrams, vocabulary_size = train_bigram_model(corpus, use_smoothing=use_smoothing)"],"metadata":{"id":"XJyg7vg6U5yT","executionInfo":{"status":"ok","timestamp":1731864554532,"user_tz":-540,"elapsed":1,"user":{"displayName":"ᄒᄒ","userId":"18137854489485594855"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## P(cake|blueberry)를 계산해보자!\n","- 단순 카운트를 이용한 계산 + 라플라스 스무딩(방법 1-2)을 이용한 보정"],"metadata":{"id":"ZZJMXmOcFxP1"}},{"cell_type":"code","source":["# 사용자로부터 단어 입력받아 확률 계산\n","w1 = 'blueberry' # input(\"Enter the first word: \")\n","w2 = 'cake' # input(\"Enter the second word: \")\n","bigram = (w1, w2)\n","\n","# 확률 조회\n","if bigram in bigram_probabilities:\n","    probability = bigram_probabilities[bigram]\n","else:\n","    # 라플라스 스무딩을 사용하는 경우 기본 확률을 스무딩된 값으로, 그렇지 않으면 0으로 처리\n","    if use_smoothing:\n","        probability = 1 / (unigrams.get(w1, 0) + vocabulary_size)\n","    else:\n","        probability = 0\n","\n","print(f\"P({w2} | {w1}) with{'out' if not use_smoothing else ''} Laplace Smoothing is: {probability}\")"],"metadata":{"id":"j7chC5kiU7a9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731864559814,"user_tz":-540,"elapsed":318,"user":{"displayName":"ᄒᄒ","userId":"18137854489485594855"}},"outputId":"f420f018-06e0-4ee7-b63d-3753029d3f78"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["P(cake | blueberry) with Laplace Smoothing is: 0.14285714285714285\n"]}]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"1w6fx0FqVt2K"}},{"cell_type":"markdown","source":["# 신경망 기반의 방법 활용 (11주차)\n","## 2-1. NNLM을 이용한 bigram 계산"],"metadata":{"id":"fbSNtqvRTuOE"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from collections import Counter\n","from torch.utils.data import Dataset, DataLoader\n","\n","# 난수 시드 고정\n","torch.manual_seed(42)"],"metadata":{"id":"4xRMmpt8I72a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731864571876,"user_tz":-540,"elapsed":5562,"user":{"displayName":"ᄒᄒ","userId":"18137854489485594855"}},"outputId":"300e9548-7f6c-47f5-a274-567e2bc42809"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x78c4c2e2d590>"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# 중복된 코드이므로 주석 처리됨\n","# 코퍼스 정의\n","# corpus = [\n","#     \"I eat a strawberry\",\n","#     \"I eat a blueberry\",\n","#     \"I eat a strawberry cake\"\n","# ]"],"metadata":{"id":"dAS3uGyhHQ1T","executionInfo":{"status":"ok","timestamp":1731864571876,"user_tz":-540,"elapsed":3,"user":{"displayName":"ᄒᄒ","userId":"18137854489485594855"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# 단어 토큰화 및 단어 사전 구축\n","words = [word for sentence in corpus for word in sentence.split()]\n","vocab = list(set(words))\n","word_to_idx = {word: i for i, word in enumerate(vocab)}\n","idx_to_word = {i: word for word, i in word_to_idx.items()}"],"metadata":{"id":"2j17hCnZHW_-","executionInfo":{"status":"ok","timestamp":1731864571877,"user_tz":-540,"elapsed":4,"user":{"displayName":"ᄒᄒ","userId":"18137854489485594855"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Hyperparameters\n","embedding_dim = 10\n","hidden_dim = 10\n","batch_size = 2\n","learning_rate = 0.01\n","epochs = 200\n","context_size = 1  # Bigram을 위해 context_size를 1로 설정"],"metadata":{"id":"vEhsuE2uHa26","executionInfo":{"status":"ok","timestamp":1731864571877,"user_tz":-540,"elapsed":3,"user":{"displayName":"ᄒᄒ","userId":"18137854489485594855"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Dataset 및 DataLoader 생성\n","class NgramDataset(Dataset):\n","    def __init__(self, corpus, context_size):\n","        self.ngrams = []\n","        for sentence in corpus:\n","            words = sentence.split()\n","            for i in range(len(words) - context_size):\n","                context = words[i:i + context_size]\n","                target = words[i + context_size]\n","                context_idxs = [word_to_idx[w] for w in context]\n","                target_idx = word_to_idx[target]\n","                self.ngrams.append((context_idxs, target_idx))\n","\n","    def __len__(self):\n","        return len(self.ngrams)\n","\n","    def __getitem__(self, idx):\n","        return torch.tensor(self.ngrams[idx][0]), torch.tensor(self.ngrams[idx][1])"],"metadata":{"id":"zRRiMk0sHdkD","executionInfo":{"status":"ok","timestamp":1731864571877,"user_tz":-540,"elapsed":3,"user":{"displayName":"ᄒᄒ","userId":"18137854489485594855"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["dataset = NgramDataset(corpus, context_size)\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"],"metadata":{"id":"6ozIJj6aHjHO","executionInfo":{"status":"ok","timestamp":1731864571877,"user_tz":-540,"elapsed":3,"user":{"displayName":"ᄒᄒ","userId":"18137854489485594855"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["각 레이어 설명\n","- Input Layer: 단어 인덱스를 임베딩 벡터로 변환하여 입력받음.\n","- Projection Layer: 여러 단어의 임베딩을 결합한 후, 이를 선형 변환하여 차원을 조정.\n","- Hidden Layer: 비선형 활성화 함수 (tanh)를 적용하여 모델이 더 복잡한 패턴을 학습함.\n","- Output Layer: 최종적으로 예측된 단어의 확률 분포를 출력."],"metadata":{"id":"nUuEpOuxJp9z"}},{"cell_type":"code","source":["# Neural Network Language Model 정의\n","class NNLM(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, context_size=1):\n","        super(NNLM, self).__init__()\n","\n","        # 임베딩 레이어 (Input Layer)\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","\n","        # Projection Layer (Linear Transformation)\n","        self.projection = nn.Linear(embedding_dim * context_size, hidden_dim)\n","\n","        # Hidden Layer (Nonlinear Transformation with tanh)\n","        self.hidden = nn.Linear(hidden_dim, hidden_dim)\n","\n","        # Output Layer\n","        self.output = nn.Linear(hidden_dim, vocab_size)\n","\n","    def forward(self, context_words):\n","        # Input Layer: 여러 단어 임베딩 후, 결합\n","        emb = self.embeddings(context_words)  # context_words의 형태는 (batch_size, context_size)\n","        emb = emb.view(emb.size(0), -1)       # (batch_size, embedding_dim * context_size)\n","\n","        # Projection Layer: 선형 변환\n","        proj = self.projection(emb)\n","\n","        # Hidden Layer: 비선형 활성화 함수 tanh 적용\n","        hidden_out = torch.tanh(self.hidden(proj))\n","\n","        # Output Layer: 다음 단어에 대한 확률 분포 계산\n","        out = self.output(hidden_out)  # (batch_size, vocab_size)\n","\n","        return out"],"metadata":{"id":"uUOHW1wBHj8S","executionInfo":{"status":"ok","timestamp":1731864574143,"user_tz":-540,"elapsed":1,"user":{"displayName":"ᄒᄒ","userId":"18137854489485594855"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# 모델, 손실 함수, 옵티마이저 초기화\n","vocab_size = len(vocab)\n","model = NNLM(vocab_size, embedding_dim, hidden_dim, context_size) # bigram=1\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)"],"metadata":{"id":"R1472ZUDHk_S","executionInfo":{"status":"ok","timestamp":1731864579974,"user_tz":-540,"elapsed":5529,"user":{"displayName":"ᄒᄒ","userId":"18137854489485594855"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# 모델 학습\n","for epoch in range(epochs):\n","    total_loss = 0\n","    for context, target in dataloader:\n","        context = context.long()\n","        target = target.long()\n","\n","        optimizer.zero_grad()\n","        outputs = model(context)\n","        loss = criterion(outputs, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","    if (epoch + 1) % 50 == 0:\n","        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")"],"metadata":{"id":"Ry7lU-NzHl0X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731864582205,"user_tz":-540,"elapsed":2233,"user":{"displayName":"ᄒᄒ","userId":"18137854489485594855"}},"outputId":"d7257d18-a5fc-4407-fc34-2ee1a66e827a"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 50/200, Loss: 1.0332\n","Epoch 100/200, Loss: 1.0133\n","Epoch 150/200, Loss: 0.9821\n","Epoch 200/200, Loss: 0.9674\n"]}]},{"cell_type":"markdown","source":["## P(cake|blueberry)를 계산해보자!\n","- NNLM을 이용하여 bigram 계산 (방법 2-1)"],"metadata":{"id":"VsbGjHq4KLML"}},{"cell_type":"code","source":["# 사용자로부터 단어 입력받아 확률 계산\n","context_words = 'blueberry'.split() # input(f\"Enter the {context_size} context words separated by space: \").split()\n","target_word = 'cake' # input(\"Enter the target word: \")\n","\n","# 입력 단어가 사전에 존재하는지 확인\n","if all(word in word_to_idx for word in context_words) and target_word in word_to_idx:\n","    with torch.no_grad():\n","        context_idxs = torch.tensor([word_to_idx[word] for word in context_words]).unsqueeze(0)\n","        outputs = model(context_idxs)\n","        probabilities = torch.softmax(outputs, dim=1)\n","        target_idx = word_to_idx[target_word]\n","        probability = probabilities[0][target_idx].item()\n","        print(f\"P({target_word} | {' '.join(context_words)}) with NNLM is: {probability}\")\n","else:\n","    print(\"One or more input words are not in the vocabulary.\")"],"metadata":{"id":"4VA_ZRcsHm7A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731864582206,"user_tz":-540,"elapsed":9,"user":{"displayName":"ᄒᄒ","userId":"18137854489485594855"}},"outputId":"03604f24-f826-4cc3-eb08-31676985132d"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["P(cake | blueberry) with NNLM is: 0.0024459303822368383\n"]}]},{"cell_type":"markdown","source":["## P(cake|redberry)를 계산해보자!\n","- 'blueberry' -> 'redberry'로 변경할 경우, 'redberry'는 corpus에 없는 단어이므로, 값이 0이됨."],"metadata":{"id":"zgOOHMIu4HJJ"}},{"cell_type":"code","source":["# 사용자로부터 단어 입력받아 확률 계산\n","context_words = 'redberry'.split() # input(f\"Enter the {context_size} context words separated by space: \").split()\n","target_word = 'cake' # input(\"Enter the target word: \")\n","\n","# 입력 단어가 사전에 존재하는지 확인\n","if all(word in word_to_idx for word in context_words) and target_word in word_to_idx:\n","    with torch.no_grad():\n","        context_idxs = torch.tensor([word_to_idx[word] for word in context_words]).unsqueeze(0)\n","        outputs = model(context_idxs)\n","        probabilities = torch.softmax(outputs, dim=1)\n","        target_idx = word_to_idx[target_word]\n","        probability = probabilities[0][target_idx].item()\n","        print(f\"P({target_word} | {' '.join(context_words)}) with NNLM is: {probability}\")\n","else:\n","    print(\"One or more input words are not in the vocabulary.\")"],"metadata":{"id":"whVXRvCS0LQ4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731864582206,"user_tz":-540,"elapsed":4,"user":{"displayName":"ᄒᄒ","userId":"18137854489485594855"}},"outputId":"04665614-fb2e-47fc-b666-d68b3be3fc42"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["One or more input words are not in the vocabulary.\n"]}]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"-zWcW9nSzwFl"}},{"cell_type":"markdown","source":["## 2-2. 코퍼스에 등장하지 않았던 bigram을 계산하는 방법\n","- UNK (Unknwon 토큰) 적용"],"metadata":{"id":"mA6manRgz8OG"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","# 난수 시드 고정\n","torch.manual_seed(42)\n","\n","# 코퍼스 정의\n","corpus = [\n","    \"I eat a strawberry\",\n","    \"I eat a blueberry\",\n","    \"I eat a strawberry cake\"\n","]\n","\n","# 단어 토큰화 및 단어 사전 구축\n","words = [word for sentence in corpus for word in sentence.split()]\n","vocab = list(set(words))\n","vocab.append(\"UNK\")  # UNK 토큰 추가\n","word_to_idx = {word: i for i, word in enumerate(vocab)}\n","idx_to_word = {i: word for word, i in word_to_idx.items()}\n","unk_idx = word_to_idx[\"UNK\"]\n","\n","# Hyperparameters\n","embedding_dim = 10\n","hidden_dim = 10\n","batch_size = 2\n","learning_rate = 0.01\n","epochs = 200\n","context_size = 1  # n-gram NNLM을 위해 context_size를 n-1로 설정 (예: trigram의 경우 2)\n","\n","# Dataset 및 DataLoader 생성\n","class NgramDataset(Dataset):\n","    def __init__(self, corpus, context_size):\n","        self.ngrams = []\n","        for sentence in corpus:\n","            words = sentence.split()\n","            for i in range(len(words) - context_size):\n","                context = words[i:i + context_size]\n","                target = words[i + context_size]\n","                context_idxs = [word_to_idx.get(w, unk_idx) for w in context]\n","                target_idx = word_to_idx.get(target, unk_idx)\n","                self.ngrams.append((context_idxs, target_idx))\n","\n","    def __len__(self):\n","        return len(self.ngrams)\n","\n","    def __getitem__(self, idx):\n","        return torch.tensor(self.ngrams[idx][0]), torch.tensor(self.ngrams[idx][1])\n","\n","dataset = NgramDataset(corpus, context_size)\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","# Neural Network Language Model 정의\n","class NNLM(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, context_size):\n","        super(NNLM, self).__init__()\n","\n","        # 임베딩 레이어 (Input Layer)\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","\n","        # Projection Layer (Linear Transformation)\n","        self.projection = nn.Linear(embedding_dim * context_size, hidden_dim)\n","\n","        # Hidden Layer (Nonlinear Transformation with tanh)\n","        self.hidden = nn.Linear(hidden_dim, hidden_dim)\n","\n","        # Output Layer\n","        self.output = nn.Linear(hidden_dim, vocab_size)\n","\n","    def forward(self, context_words):\n","        # Input Layer: 여러 단어 임베딩 후, 결합\n","        emb = self.embeddings(context_words)  # context_words의 형태는 (batch_size, context_size)\n","        emb = emb.view(emb.size(0), -1)       # (batch_size, embedding_dim * context_size)\n","\n","        # Projection Layer: 선형 변환\n","        proj = self.projection(emb)\n","\n","        # Hidden Layer: 비선형 활성화 함수 tanh 적용\n","        hidden_out = torch.tanh(self.hidden(proj))\n","\n","        # Output Layer: 다음 단어에 대한 확률 분포 계산\n","        out = self.output(hidden_out)  # (batch_size, vocab_size)\n","\n","        return out\n","\n","# 모델, 손실 함수, 옵티마이저 초기화\n","vocab_size = len(vocab)\n","model_unk = NNLM(vocab_size, embedding_dim, hidden_dim, context_size)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model_unk.parameters(), lr=learning_rate)\n","\n","# 모델 학습\n","for epoch in range(epochs):\n","    total_loss = 0\n","    for context, target in dataloader:\n","        context = context.long()\n","        target = target.long()\n","\n","        optimizer.zero_grad()\n","        outputs = model_unk(context)\n","        loss = criterion(outputs, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","    if (epoch + 1) % 50 == 0:\n","        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")"],"metadata":{"id":"MH7epxL0zTM2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731864584774,"user_tz":-540,"elapsed":2139,"user":{"displayName":"ᄒᄒ","userId":"18137854489485594855"}},"outputId":"476e26bc-2e0f-41c6-8f0f-bed9c87632e3"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 50/200, Loss: 1.0212\n","Epoch 100/200, Loss: 0.9894\n","Epoch 150/200, Loss: 0.9687\n","Epoch 200/200, Loss: 0.9700\n"]}]},{"cell_type":"code","source":["# 사용자로부터 단어 입력받아 확률 계산\n","context_words = 'redberry'.split() # input(f\"Enter the {context_size} context words separated by space: \").split()\n","target_word = 'cake' # input(\"Enter the target word: \")\n","\n","# 입력 단어가 사전에 존재하지 않으면 UNK 인덱스 사용\n","context_idxs = [word_to_idx.get(word, unk_idx) for word in context_words]\n","target_idx = word_to_idx.get(target_word, unk_idx)\n","\n","with torch.no_grad():\n","    context_idxs = torch.tensor(context_idxs).unsqueeze(0)\n","    outputs = model_unk(context_idxs)\n","    probabilities = torch.softmax(outputs, dim=1)\n","    probability = probabilities[0][target_idx].item()\n","    print(f\"P({target_word} | {' '.join(context_words)}) with NNLM is: {probability}\")"],"metadata":{"id":"G75VVDwx0GMV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731864584774,"user_tz":-540,"elapsed":3,"user":{"displayName":"ᄒᄒ","userId":"18137854489485594855"}},"outputId":"836edd10-ea33-48a9-b60e-2d96f77e5309"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["P(cake | redberry) with NNLM is: 0.019036589190363884\n"]}]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"utyL0WSdKBGK"}},{"cell_type":"markdown","source":["# **[[실습 11-2-1]]** 통계 기반 방법과 신경망 기반의 방법 예시 만들기\n","- (아래와 같은 코퍼스가 있을 때) **코퍼스에 등장하지 않은 두 단어(bigram)**가 포함되어 있는 경우를 살펴볼 수 있는 예시를 만들어 보시오.\n","- 참고 사항\n","  - 위에서는 1-1 vs 1-2를 비교하기 위해, **'blueberry cake'**라는 예제를 살펴보았고,\n","  - 2-1 vs 2-2를 비교하기 위해, **'redberry cake'**라는 예제를 살펴보았음.\n","  - 이와 같이 (위에서 살펴본 것과 다른) 두 단어로 이루어진 예시(bigram)를 적절히 만들어 보고 코드를 실행하시오.\n","\n","```\n","# 코퍼스 정의\n","corpus = [\n","    \"I eat a strawberry\",\n","    \"I eat a blueberry\",\n","    \"I eat a strawberry cake\"\n","]\n","```\n","\n","### 다음의 코드에서 *'???' ### 이 부분을 변경해 보시오.* <- **???** 부분을 변경하여 코드를 실행하면 됨.\n"],"metadata":{"id":"Lg6rh32DrEGe"}},{"cell_type":"markdown","source":["## 1-1 방법\n","- 통계 기반의 방법, 라플라스 스무딩 적용 X\n","- 예상되는 코드 실행 결과: P(w2 | w1) without Laplace Smoothing is: 0"],"metadata":{"id":"M_rk_dcj6N3k"}},{"cell_type":"code","source":["use_smoothing = False\n","\n","# 사용자로부터 단어 입력받아 확률 계산\n","w1 = 'strawberry' ### 이 부분을 변경해 보시오. ex) bluberry\n","w2 = 'jelly' ### 이 부분을 변경해 보시오. ex) cake\n","bigram = (w1, w2)\n","\n","# 확률 조회\n","if bigram in bigram_probabilities:\n","    probability = bigram_probabilities[bigram]\n","else:\n","    # 라플라스 스무딩을 사용하는 경우 기본 확률을 스무딩된 값으로, 그렇지 않으면 0으로 처리\n","    if use_smoothing:\n","        probability = 1 / (unigrams.get(w1, 0) + vocabulary_size)\n","    else:\n","        probability = 0\n","\n","print(f\"P({w2} | {w1}) with{'out' if not use_smoothing else ''} Laplace Smoothing is: {probability}\")"],"metadata":{"id":"JhgoJICG3l9V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731865157071,"user_tz":-540,"elapsed":312,"user":{"displayName":"ᄒᄒ","userId":"18137854489485594855"}},"outputId":"20800b6c-cdc8-40e8-99ed-a082baac76d7"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["P(jelly | strawberry) without Laplace Smoothing is: 0\n"]}]},{"cell_type":"markdown","source":["## 1-2 방법\n","- 통계 기반의 방법, 라플라스 스무딩 적용 O\n","- 예상되는 코드 실행 결과: P(w2 | w1) with Laplace Smoothing is: (확률 값)\n","\n"],"metadata":{"id":"vfbCqEmv6TUQ"}},{"cell_type":"code","source":["use_smoothing = True\n","\n","# 사용자로부터 단어 입력받아 확률 계산\n","w1 = 'strawberry' ### 이 부분을 변경해 보시오. ex) bluberry\n","w2 = 'jelly' ### 이 부분을 변경해 보시오. ex) cake\n","bigram = (w1, w2)\n","\n","# 확률 조회\n","if bigram in bigram_probabilities:\n","    probability = bigram_probabilities[bigram]\n","else:\n","    # 라플라스 스무딩을 사용하는 경우 기본 확률을 스무딩된 값으로, 그렇지 않으면 0으로 처리\n","    if use_smoothing:\n","        probability = 1 / (unigrams.get(w1, 0) + vocabulary_size)\n","    else:\n","        probability = 0\n","\n","print(f\"P({w2} | {w1}) with{'out' if not use_smoothing else ''} Laplace Smoothing is: {probability}\")"],"metadata":{"id":"5t48wkEWrH4n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731865173855,"user_tz":-540,"elapsed":309,"user":{"displayName":"ᄒᄒ","userId":"18137854489485594855"}},"outputId":"b84dca12-d3d9-4992-fcd9-d18926f61a13"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["P(jelly | strawberry) with Laplace Smoothing is: 0.125\n"]}]},{"cell_type":"markdown","source":["## 2-1 방법\n","- 신경망 기반의 방법, UNK 토큰 적용 X\n","- 예상되는 코드 실행 결과: One or more input words are not in the vocabulary."],"metadata":{"id":"G9a7oxel6edB"}},{"cell_type":"code","source":["# 사용자로부터 단어 입력받아 확률 계산\n","context_words = 'raspberry'.split() ### 이 부분을 변경해 보시오. ex) redberry\n","target_word = 'jelly' ### 이 부분을 변경해 보시오. ex) cake\n","\n","# 입력 단어가 사전에 존재하는지 확인\n","if all(word in word_to_idx for word in context_words) and target_word in word_to_idx:\n","    with torch.no_grad():\n","        context_idxs = torch.tensor([word_to_idx[word] for word in context_words]).unsqueeze(0)\n","        outputs = model(context_idxs)\n","        probabilities = torch.softmax(outputs, dim=1)\n","        target_idx = word_to_idx[target_word]\n","        probability = probabilities[0][target_idx].item()\n","        print(f\"P({target_word} | {' '.join(context_words)}) with NNLM is: {probability}\")\n","else:\n","    print(\"One or more input words are not in the vocabulary.\")"],"metadata":{"id":"DCXkp2NF3g8Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731865222838,"user_tz":-540,"elapsed":306,"user":{"displayName":"ᄒᄒ","userId":"18137854489485594855"}},"outputId":"399efd5b-dde7-4388-811e-b6d3bdf27549"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["One or more input words are not in the vocabulary.\n"]}]},{"cell_type":"markdown","source":["## 2-2 방법\n","- 신경망 기반의 방법, UNK 토큰 적용 O\n","- 예상되는 코드 실행 결과: P(target_word | context_words) with NNLM is: (확률 값)\n"],"metadata":{"id":"_YaJ-L5A6MSO"}},{"cell_type":"code","source":["# 사용자로부터 단어 입력받아 확률 계산\n","context_words = 'raspberry'.split() ### 이 부분을 변경해 보시오, ex) redberry\n","target_word = 'jelly' ### 이 부분을 변경해 보시오. ex) cake\n","\n","# 입력 단어가 사전에 존재하지 않으면 UNK 인덱스 사용\n","context_idxs = [word_to_idx.get(word, unk_idx) for word in context_words]\n","target_idx = word_to_idx.get(target_word, unk_idx)\n","\n","with torch.no_grad():\n","    context_idxs = torch.tensor(context_idxs).unsqueeze(0)\n","    outputs = model_unk(context_idxs)\n","    probabilities = torch.softmax(outputs, dim=1)\n","    probability = probabilities[0][target_idx].item()\n","    print(f\"P({target_word} | {' '.join(context_words)}) with NNLM is: {probability}\")"],"metadata":{"id":"_LqZ2VaK3vr1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731865273958,"user_tz":-540,"elapsed":433,"user":{"displayName":"ᄒᄒ","userId":"18137854489485594855"}},"outputId":"0203d55d-6ff4-4ffd-a7ec-09233b0895f4"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["P(jelly | raspberry) with NNLM is: 0.0014913304476067424\n"]}]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"7T70Ni8MrN0a"}},{"cell_type":"markdown","source":["# [파일] -> [다운로드] -> [.ipynb 다운로드]"],"metadata":{"id":"ev3k3TuErQ49"}}]}